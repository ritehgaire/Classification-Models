{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6617187c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00       661\n",
      "\n",
      "    accuracy                           1.00       661\n",
      "   macro avg       1.00      1.00      1.00       661\n",
      "weighted avg       1.00      1.00      1.00       661\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00       661\n",
      "\n",
      "    accuracy                           1.00       661\n",
      "   macro avg       1.00      1.00      1.00       661\n",
      "weighted avg       1.00      1.00      1.00       661\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "# Function to download and extract datasets\n",
    "def fetch_spam_datasets():\n",
    "    # URL of the datasets\n",
    "    url = \"https://spamassassin.apache.org/old/publiccorpus/\"\n",
    "\n",
    "    # Directory to save the datasets\n",
    "    data_dir = \"spam_datasets\"\n",
    "\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    # List of datasets to download\n",
    "    datasets = [\n",
    "        \"20021010_easy_ham.tar.bz2\",\n",
    "        \"20021010_hard_ham.tar.bz2\",\n",
    "        \"20021010_spam.tar.bz2\",\n",
    "    ]\n",
    "\n",
    "    # Download and extract datasets\n",
    "    for dataset in datasets:\n",
    "        dataset_url = url + dataset\n",
    "        dataset_path = os.path.join(data_dir, dataset)\n",
    "        urllib.request.urlretrieve(dataset_url, dataset_path)\n",
    "        with tarfile.open(dataset_path, \"r:bz2\") as tar:\n",
    "            tar.extractall(data_dir)\n",
    "\n",
    "# Step 1: Download and extract datasets\n",
    "fetch_spam_datasets()\n",
    "\n",
    "# Step 2: Load and preprocess datasets\n",
    "def load_emails(data_dir):\n",
    "    emails = []\n",
    "    labels = []\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                content = f.read().decode(errors=\"ignore\")\n",
    "                emails.append(content)\n",
    "                labels.append(1 if \"spam\" in root else 0)\n",
    "    return emails, labels\n",
    "\n",
    "data_dir = \"spam_datasets\"\n",
    "emails, labels = load_emails(data_dir)\n",
    "\n",
    "# Step 3: Split datasets into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(emails, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Data Preparation Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "data_prep_pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(\n",
    "        lowercase=True,\n",
    "        stop_words='english',\n",
    "        strip_accents='unicode',\n",
    "        token_pattern=r'\\b\\w\\w+\\b',  # Match words containing 2 or more alphanumeric characters\n",
    "        max_df=0.95,  # Ignore terms that appear in more than 95% of the documents\n",
    "        min_df=2,  # Ignore terms that appear in less than 2 documents\n",
    "        max_features=1000  # Limit the number of features to 1000\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Step 5: Feature Vector Representation\n",
    "X_train_features = data_prep_pipeline.fit_transform(X_train)\n",
    "X_test_features = data_prep_pipeline.transform(X_test)\n",
    "\n",
    "# Step 6: Classifier Selection and Training\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "naive_bayes_classifier.fit(X_train_features, y_train)\n",
    "\n",
    "# Step 7: Evaluation\n",
    "y_pred = naive_bayes_classifier.predict(X_test_features)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 8: Hyperparameter Tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'alpha': [0.01, 0.1, 1.0]}  # Example hyperparameters for MultinomialNB\n",
    "grid_search = GridSearchCV(naive_bayes_classifier, param_grid, cv=5)\n",
    "grid_search.fit(X_train_features, y_train)\n",
    "best_classifier = grid_search.best_estimator_\n",
    "y_pred_tuned = best_classifier.predict(X_test_features)\n",
    "print(classification_report(y_test, y_pred_tuned))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1132ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
